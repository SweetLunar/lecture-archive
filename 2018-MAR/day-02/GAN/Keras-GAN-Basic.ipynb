{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division                                          \n",
    "from __future__ import print_function                                           \n",
    "from __future__ import absolute_import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "from keras import initializers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 200\n",
    "BATCH_SIZE = 100\n",
    "OPTIMIZER = Adam(0.0002, 0.5)\n",
    "KERNEL_INIT = initializers.RandomNormal(stddev=0.02)\n",
    "\n",
    "NUM_HIDDEN = 256\n",
    "NOISE_DIMS = 128\n",
    "IMAGE_DIMS = 28 * 28\n",
    "USE_DCGAN = True\n",
    "\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_noise(num, dim_size):\n",
    "     return np.random.normal(0, 1, (num, dim_size))\n",
    "\n",
    "def pick_images(dataset, labelset, batch_size):\n",
    "    indexes = np.random.randint(0, dataset.shape[0], batch_size)\n",
    "    images = dataset[indexes]\n",
    "    labels = labelset[indexes]\n",
    "    return images, labels\n",
    "\n",
    "def load_dataset():\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = X_train.reshape(-1, IMAGE_DIMS)\n",
    "    print('X_train value range:', np.min(X_train), np.max(X_train))\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    return X_train, y_train\n",
    "\n",
    "def generator():\n",
    "    g = Sequential()\n",
    "    if USE_DCGAN :\n",
    "        g.add(Dense(NUM_HIDDEN, input_dim=NOISE_DIMS, kernel_initializer=KERNEL_INIT))\n",
    "        g.add(LeakyReLU(0.2))\n",
    "        g.add(Dense(512))\n",
    "        g.add(LeakyReLU(0.2))\n",
    "        g.add(Dense(1024))\n",
    "        g.add(LeakyReLU(0.2))\n",
    "        g.add(Dense(IMAGE_DIMS, activation='tanh'))\n",
    "        g.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)\n",
    "    else:\n",
    "        g.add(Dense(NUM_HIDDEN, input_dim=NOISE_DIMS, kernel_initializer=KERNEL_INIT))\n",
    "        g.add(Activation(\"relu\"))\n",
    "        g.add(Dense(IMAGE_DIMS, activation=\"tanh\"))\n",
    "        g.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)\n",
    "    return g\n",
    "\n",
    "def discriminator():\n",
    "    d = Sequential()\n",
    "    if USE_DCGAN :\n",
    "        d.add(Dense(1024, input_dim=IMAGE_DIMS, kernel_initializer=KERNEL_INIT))\n",
    "        d.add(LeakyReLU(0.2))\n",
    "        d.add(Dropout(0.3))\n",
    "        d.add(Dense(512))\n",
    "        d.add(LeakyReLU(0.2))\n",
    "        d.add(Dropout(0.3))\n",
    "        d.add(Dense(256))\n",
    "        d.add(LeakyReLU(0.2))\n",
    "        d.add(Dropout(0.3))\n",
    "        d.add(Dense(1, activation='sigmoid'))\n",
    "        d.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)\n",
    "    else:\n",
    "        d.add(Dense(NUM_HIDDEN, input_dim=IMAGE_DIMS, kernel_initializer=KERNEL_INIT))\n",
    "        d.add(Activation(\"relu\"))\n",
    "        # d.add(Dense(1, activation\"sigmoid\", input_dim=NUM_HIDDEN))\n",
    "        d.add(Dense(1, activation=\"sigmoid\"))\n",
    "        d.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)\n",
    "    return d\n",
    "\n",
    "def gan_model(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    noise_tensor = Input(shape=(NOISE_DIMS,))\n",
    "    image_gen_tensor = generator(noise_tensor)\n",
    "    validity_tensor = discriminator(image_gen_tensor)\n",
    "\n",
    "    gan = Model(inputs=noise_tensor, outputs=validity_tensor)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)\n",
    "    return gan\n",
    "\n",
    "def train():\n",
    "    g = generator()\n",
    "    d = discriminator()\n",
    "    gan = gan_model(g, d);\n",
    "    \n",
    "    dis_losses, gen_losses = [], []\n",
    "    X_train, y_train = load_dataset()\n",
    "    batch_count = int(X_train.shape[0] / BATCH_SIZE)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        time_begin = time.time()\n",
    "        for batch in range(batch_count):\n",
    "            noise = make_noise(BATCH_SIZE, NOISE_DIMS)\n",
    "            gen_images = g.predict(noise)\n",
    "            real_images, labels = pick_images(X_train, y_train, BATCH_SIZE)\n",
    "            \n",
    "            X = np.concatenate((real_images, gen_images))\n",
    "            y_dis = np.zeros(2*BATCH_SIZE)\n",
    "            y_dis[:BATCH_SIZE] = 0.96  \n",
    "            \n",
    "            d.trainable = True\n",
    "            dis_loss = d.train_on_batch(X, y_dis)\n",
    "\n",
    "            noise = make_noise(BATCH_SIZE, NOISE_DIMS)\n",
    "            y_gen = np.ones(BATCH_SIZE)\n",
    "            d.trainable = False\n",
    "            gen_loss = gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "        dis_losses.append(dis_loss)\n",
    "        gen_losses.append(gen_loss)\n",
    "        print (\"%d dis-loss: %f  gen-loss: %f  time-elapsed %f\" % (epoch, dis_loss, gen_loss, time.time() - time_begin))\n",
    "        on_epoch_completion(epoch, g, dis_losses, gen_losses)\n",
    "\n",
    "def on_epoch_completion(epoch, generator, dis_loss, gen_loss):\n",
    "    if epoch % 20 == 0:\n",
    "        images = generate_images(generator, 100)\n",
    "        show_images(images)\n",
    "\n",
    "def generate_images(generator, num):\n",
    "    noise = make_noise(num, NOISE_DIMS)\n",
    "    images = generator.predict(noise)\n",
    "    images = images.reshape(-1, 28, 28)\n",
    "    return images\n",
    "\n",
    "def show_images(images, dim=(10, 10), figsize=(10, 10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(epoch):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(dLosses, label='Discriminitive loss')\n",
    "    plt.plot(gLosses, label='Generative loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('images/gan_loss_epoch_%d.png' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
